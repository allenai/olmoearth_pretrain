---
description: when someone asks you to do a PR review of code, pull request review, review this, or check a PR
alwaysApply: false
---

# PR Review Guidelines

Reviews should be kind, specific, and actionable. Always distinguish severity clearly.

## Severity Labels

Prefix every comment with one of:

- **[MUST]** — blocks merge. Correctness bugs, broken tests, API breakage, checkpoint incompatibility, silent behavior changes.
- **[SHOULD]** — strong preference, easy to fix, reviewer will likely re-request. DRY violations, missing tests for non-trivial logic, unclear naming.
- **[NIT]** — style/polish, take it or leave it. Don't block on nits.
- **[QUESTION]** — genuine uncertainty, not a request to change. Ask before flagging as must/should.

Always give a concrete example of what you want to see, not just what's wrong.

---

## Code Quality Checklist

### DRY & Modularity
- If the same logic appears in 2+ places **in library code** (`olmoearth_pretrain/`), flag it.
- Config dataclasses should compose, not duplicate fields.
- **Scripts are an exception.** It is fine — and often preferable — to copy config values explicitly in experiment scripts rather than abstracting them. Scripts are meant to be read as self-contained experiment records. A reader should be able to understand an experiment fully without tracing imports across multiple files.

```python
# fine in a script — explicit is better than a shared helper that hides what changed
encoder_config = EncoderConfig(
    embedding_size=768,
    num_heads=12,
    depth=12,
    use_flash_attn=True,
    use_linear_patch_embed=True,
)

# bad in library code (olmoearth_pretrain/) — same logic copy-pasted across Loss classes
```

- Flag shared abstractions between scripts that make individual scripts harder to read in isolation. The shared `script.py` builders are a floor, not a ceiling — scripts can and should repeat values when that makes the experiment clearer.

### Explicit > Clever Python
- Avoid implicit behavior that requires knowing internals to understand.
- Prefer named args over positional for calls with 3+ args.
- `if x is not None` over `if x` when `None` vs falsy matters.
- No list comprehensions that span 3+ logical operations — break into a loop.

```python
# bad
result = {k: v for d in [a, b] for k, v in d.items() if v and not k.startswith("_")}

# good
result = {}
for d in [a, b]:
    for k, v in d.items():
        if v and not k.startswith("_"):
            result[k] = v
```

### Extensibility
- New model variants → new `Config` subclass, never modify the base class.
- New loss / masking strategies → register in the registry, don't branch on strings in forward().
- Flag any `if experiment_name == "foo":` style branching — it should be a config field.

### Experiment Scripts
- New experiments go in a **new script file** under `scripts/vnext/<folder>/` or `scripts/official/ablations/`.
- **Never modify `scripts/official/base.py`** (or other canonical scripts) for an experiment — it's the reference baseline.
- Scripts are allowed to be verbose and repetitive. A full copy of a builder with one value changed is fine — the goal is that the script reads as a complete, honest description of the experiment. Prefer clarity over cleverness.
- The only thing that should be shared between scripts is code from `script.py` that represents the stable training infrastructure (dataset, dataloader, trainer callbacks). Model architecture and loss config should be written out explicitly.

### Config Pattern
- All configurable components follow `@dataclass Config` → `.build()`. Flag any component that takes raw kwargs instead.
- `use_X: bool` flags should be in the `Config` dataclass, not in the `__init__` signature only.
- New fields on existing `Config` classes need a safe default so old checkpoints still deserialize correctly. Flag any field without a default.

---

## Testing Checklist

- Non-trivial logic (new loss, new masking strategy, new model component) needs a unit test.
- Test files mirror module paths exactly: `olmoearth_pretrain/nn/foo.py` → `tests/unit/nn/test_foo.py`. The directory structure under `tests/unit/` matches the structure under `olmoearth_pretrain/` one-to-one. Never put a test for `nn/foo.py` at `tests/unit/test_foo.py` — it belongs in the `nn/` subdirectory.
- Integration tests for anything touching checkpoint save/load or the train loop.
- **Never comment out existing tests.** Flag this as [MUST].
- If a test uses mocks to avoid running the actual logic, flag as [MUST] unless it's purely I/O.

---

## Documentation Checklist

- Public functions/classes need docstrings with Args/Returns if non-obvious.
- Docstrings should lead with **what** the function does, not **how**. Implementation details belong in inline comments, not the docstring.

```python
# bad — describes implementation, not behavior
def unmask(self, x, indices, mask):
    """Uses scatter ops and zero-padding to avoid boolean indexing."""

# good — describes what the caller needs to know
def unmask(self, x, indices, mask):
    """Restore masked tokens to their original positions in the sequence."""
```

- If a new config field changes behavior, the docstring on the `Config` class should say so.
- If the PR adds a new script, check for a README or inline docstring explaining launch usage.
- If backward-compat behavior changes (e.g. checkpoint loading), flag if there's no migration note.
- Check `CHANGELOG.md` or equivalent if the repo tracks one.

---

## Common Patterns to Flag

### Backward Compatibility
Flag any change to an existing `Config` dataclass field default or type — old serialized configs may break. Ask if there's a migration path.

### GPU Sync Points
In training code, flag:
- `tensor.item()` inside a training loop (CPU sync)
- Boolean indexing (`x[mask]`) without explanation — triggers `nonzero()` + sync
- `if tensor.any():` without `.item()` clarity

### Commented-Out Code
Remove it or track it in a TODO with a GitHub issue link. Commented-out `compile` calls, old FSDP sharding strategies, etc. should not persist.

```python
# bad
# self.compile(mode="max-autotune", dynamic=False)  # try again later
for block in self.blocks:
    block.apply_compile()

# good — just the working code
for block in self.blocks:
    block.apply_compile()
```

### Naming
- Class names should say what they are, not how they differ from a base (`ModalityPatchDiscriminationLossVec` not `ModalityPatchDiscriminationLossNew2`).
- Test function names should say what they test (`test_unmask_preserves_token_order` not `test_unmask`).
- Registry keys should match class names in spirit (`"modality_patch_discrimination_new_vec"` → `ModalityPatchDiscriminationLossVec`).

---

## Review Format

```
## Summary
One paragraph: what does this PR do, does it achieve that goal?

## Must Fix
- [MUST] `olmoearth_pretrain/nn/flexi_vit.py:442` — <issue>
  <concrete fix or snippet>

## Should Fix
- [SHOULD] `scripts/official/base.py:32` — <issue>
  <concrete fix or snippet>

## Nits
- [NIT] ...

## Questions
- [QUESTION] ...

## Docs / Tests
- Missing unit test for X
- Docstring for Y doesn't cover the new `use_Z` flag
```

---

## Quick Reference: Things Often Missed in This Repo

- New `Config` field without default → old checkpoints fail to deserialize (OmegaConf fills missing keys from schema default, which may be wrong)
- `num_masked_views` in dataloader must match the train module (1 for LatentMIM, 2 for ContrastiveLatentMIM)
- `apply_compile()` and `apply_fsdp()` both need updating when adding new submodules to a model
- Eval tasks in `build_trainer_config` use hardcoded `eval_interval` — flag if it's clearly wrong for the run length
- `wandb.project` left as the default from `script.py` in a new experiment script (it'll log to the wrong project)
